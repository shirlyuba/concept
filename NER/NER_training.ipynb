{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from ner.utils import download_untar\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ner.network import NER\n",
    "\n",
    "\n",
    "from os.path import join\n",
    "folder = join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dict()\n",
    "\n",
    "\n",
    "with open( join(folder, 'NER_Pers.txt'), encoding='utf-8') as f:\n",
    "    xy_list = list()\n",
    "    tokens = list()\n",
    "    tags = list()\n",
    "    val = list()\n",
    "    for line in f:\n",
    "        items = line.split()\n",
    "        if len(items) > 1 and '-DOCSTART-' not in items[0]:\n",
    "            token, tag = items\n",
    "            if token[0].isdigit():\n",
    "                tokens.append('#')\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        elif len(tokens) > 0:\n",
    "            xy_list.append((tokens, tags,))\n",
    "            tokens = list()\n",
    "            tags = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'], ost = train_test_split(xy_list,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['valid'], dataset_dict['test'] = train_test_split(ost,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddins...\n"
     ]
    }
   ],
   "source": [
    "from ner.corpus import Corpus\n",
    "corp = Corpus(dataset_dict, embeddings_file_path='lenta_lower_100.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "Now we have to create the Neural Network. To do so we use NER class from the network module. The NER constructor takes the following arguments:\n",
    "\n",
    "    token_embeddings_dim - token embeddings dimensionality (must agree with embeddings if they are provided)\n",
    "    char_embeddings_dim - character embeddings dimensionality \n",
    "    use_crf - whether to use Conditional Random Fields on the top (suggested to always use True)\n",
    "    use_capitalization - whethere to include capitalization binary features to the input of the network.\n",
    "                         If True than binary feature indicating whether the word starts with capital letter\n",
    "                         will be concatenated to the word embeddings.\n",
    "    n_filters - list of output feature dimensionality for each layer. For [100, 200] there will be two\n",
    "                layers with 100 and 200 number of units respectively.\n",
    "    embeddings_dropout - whether to use dropout on embeddings\n",
    "    \n",
    "There are special type of argument determinig what type of net to build:\n",
    "    \n",
    "    net_type - could be one of the following 'cnn', 'rnn', and 'cnn_highway'\n",
    "    \n",
    "For each net type there are a number of optional arguments. For convolutional neural networks ('cnn' and 'cnn_highway' net types) there are:\n",
    "\n",
    "    filter_width - width of the convolutional filter (number of tokens under the filter)\n",
    "    use_batch_norm - if True each layer will be provided with batch normalization\n",
    "\n",
    "For 'rnn' net there is\n",
    "    \n",
    "    cell_type - could be lstm or gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training\n",
    "To train the network the following parameters must be specified:\n",
    "\n",
    "    dropout_rate - probability of dropping the hidden state a value from 0 to 1. 0.5 Works well\n",
    "                   in most of the cases\n",
    "    epochs - number of epochs (10 - 100 typical)\n",
    "    learning_rate: learning rate (0.01 - 0.0001 typical)\n",
    "    batch_size: number of samples in the batch (4 - 64 typical)\n",
    "    learning_rate_decay - multiple factor of decreasing learning rate every epoch (1 - 0.5 typical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "ConvNet 185088\n",
      "Embeddings 3800625\n",
      "Classifier 1028\n",
      "transitions:0 16\n",
      "Total number of parameters equal 3986757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1051 phrases; correct: 994.\n",
      "\n",
      "precision:  94.58%; recall:  89.87%; FB1:  92.17\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1132 phrases; correct: 1048.\n",
      "\n",
      "precision:  92.58%; recall:  94.76%; FB1:  93.66\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1058 phrases; correct: 943.\n",
      "\n",
      "precision:  89.13%; recall:  85.26%; FB1:  87.15\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1161 phrases; correct: 1048.\n",
      "\n",
      "precision:  90.27%; recall:  94.76%; FB1:  92.46\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1037 phrases; correct: 939.\n",
      "\n",
      "precision:  90.55%; recall:  84.90%; FB1:  87.63\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Eval on valid:\n",
      "processed 22627 tokens with 1106 phrases; found: 1364 phrases; correct: 974.\n",
      "\n",
      "precision:  71.41%; recall:  88.07%; FB1:  78.87\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 937 phrases; correct: 877.\n",
      "\n",
      "precision:  93.60%; recall:  79.29%; FB1:  85.85\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1142 phrases; correct: 1029.\n",
      "\n",
      "precision:  90.11%; recall:  93.04%; FB1:  91.55\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1141 phrases; correct: 1025.\n",
      "\n",
      "precision:  89.83%; recall:  92.68%; FB1:  91.23\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1219 phrases; correct: 1052.\n",
      "\n",
      "precision:  86.30%; recall:  95.12%; FB1:  90.49\n",
      "\n",
      "\n",
      "Eval on train:\n",
      "processed 182573 tokens with 8465 phrases; found: 8487 phrases; correct: 8456.\n",
      "\n",
      "precision:  99.63%; recall:  99.89%; FB1:  99.76\n",
      "\n",
      "\tPER: precision:  99.63%; recall:  99.89%; F1:  99.76 8487\n",
      "\n",
      "\n",
      "Eval on valid:\n",
      "processed 22628 tokens with 1106 phrases; found: 1219 phrases; correct: 1052.\n",
      "\n",
      "precision:  86.30%; recall:  95.12%; FB1:  90.49\n",
      "\n",
      "\tPER: precision:  86.30%; recall:  95.12%; F1:  90.49 1219\n",
      "\n",
      "\n",
      "Eval on test:\n",
      "processed 20927 tokens with 1018 phrases; found: 1142 phrases; correct: 971.\n",
      "\n",
      "precision:  85.03%; recall:  95.38%; FB1:  89.91\n",
      "\n",
      "\tPER: precision:  85.03%; recall:  95.38%; F1:  89.91 1142\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = NER(corp, token_embeddings_dim=100, use_crf=True,char_embeddings_dim=25,\n",
    "         concat_embeddings=True, use_char_embeddins=True)\n",
    "learning_params = {'dropout_rate': 0.5,\n",
    "                   'epochs': 10,\n",
    "                   'learning_rate': 0.005,\n",
    "                   'batch_size': 1}\n",
    "\n",
    "results = net.fit(**learning_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
