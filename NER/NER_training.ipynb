{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.utils import download_untar\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ner.network import NER\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "from ner.utils import tokenize, lemmatize\n",
    "import json\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c33d710eeb4c09abc9de193bdc03f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with open(join('.', 'NER_concept.txt'), encoding='utf-8') as f:\n",
    "#     xy_list = list()\n",
    "#     tokens = list()\n",
    "#     tags = list()\n",
    "#     val = list()\n",
    "#     for line in tqdm_notebook(f):\n",
    "#         items = line.split()\n",
    "#         if len(items) > 1 and '-DOCSTART-' not in items[0]:\n",
    "#             token, tag = items\n",
    "#             token_ = tokenize(token)\n",
    "#             if len(token_) > 0:\n",
    "#                 token = token_[0]\n",
    "#             if token[0].isdigit():\n",
    "#                 tokens.append('#')\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#             tags.append(tag)\n",
    "#         elif len(tokens) > 0:\n",
    "#             xy_list.append((tokens, tags,))\n",
    "#             tokens = list()\n",
    "#             tags = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_list = np.load('NER_set_simple.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_list = np.append(xy_list, np.load('NER_set_conll_2003.npy'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'], ost = train_test_split(xy_list,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['valid'], dataset_dict['test'] = train_test_split(ost,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('NER_set_simple.npy', xy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('NER_set_conll_2003.npy', xy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddins...\n"
     ]
    }
   ],
   "source": [
    "from ner.corpus import Corpus\n",
    "corp = Corpus(dataset_dict, embeddings_file_path='model185.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_model/params.json') as f:\n",
    "    network_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_model/my_model\n"
     ]
    }
   ],
   "source": [
    "net = NER(corp, verbouse=False, pretrained_model_filepath='my_model/my_model', **network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "Embeddings 5826150\n",
      "ConvNet 338688\n",
      "Classifier 1542\n",
      "transitions:0 36\n",
      "Total number of parameters equal 6166416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 277 phrases; correct: 173.\n",
      "\n",
      "precision:  62.45%; recall:  58.25%; FB1:  60.28\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 224 phrases; correct: 177.\n",
      "\n",
      "precision:  79.02%; recall:  59.60%; FB1:  67.95\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 239 phrases; correct: 185.\n",
      "\n",
      "precision:  77.41%; recall:  62.29%; FB1:  69.03\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 329 phrases; correct: 158.\n",
      "\n",
      "precision:  48.02%; recall:  53.20%; FB1:  50.48\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 231 phrases; correct: 161.\n",
      "\n",
      "precision:  69.70%; recall:  54.21%; FB1:  60.98\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 276 phrases; correct: 190.\n",
      "\n",
      "precision:  68.84%; recall:  63.97%; FB1:  66.32\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 319 phrases; correct: 190.\n",
      "\n",
      "precision:  59.56%; recall:  63.97%; FB1:  61.69\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 313 phrases; correct: 197.\n",
      "\n",
      "precision:  62.94%; recall:  66.33%; FB1:  64.59\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 226 phrases; correct: 160.\n",
      "\n",
      "precision:  70.80%; recall:  53.87%; FB1:  61.19\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 240 phrases; correct: 166.\n",
      "\n",
      "precision:  69.17%; recall:  55.89%; FB1:  61.82\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 381 phrases; correct: 174.\n",
      "\n",
      "precision:  45.67%; recall:  58.59%; FB1:  51.33\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 197 phrases; correct: 136.\n",
      "\n",
      "precision:  69.04%; recall:  45.79%; FB1:  55.06\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 214 phrases; correct: 154.\n",
      "\n",
      "precision:  71.96%; recall:  51.85%; FB1:  60.27\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 211 phrases; correct: 151.\n",
      "\n",
      "precision:  71.56%; recall:  50.84%; FB1:  59.45\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 201 phrases; correct: 148.\n",
      "\n",
      "precision:  73.63%; recall:  49.83%; FB1:  59.44\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 248 phrases; correct: 164.\n",
      "\n",
      "precision:  66.13%; recall:  55.22%; FB1:  60.18\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 180 phrases; correct: 145.\n",
      "\n",
      "precision:  80.56%; recall:  48.82%; FB1:  60.80\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 206 phrases; correct: 143.\n",
      "\n",
      "precision:  69.42%; recall:  48.15%; FB1:  56.86\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 313 phrases; correct: 180.\n",
      "\n",
      "precision:  57.51%; recall:  60.61%; FB1:  59.02\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 258 phrases; correct: 154.\n",
      "\n",
      "precision:  59.69%; recall:  51.85%; FB1:  55.50\n",
      "\n",
      "\n",
      "Eval on train:\n",
      "processed 83191 tokens with 2181 phrases; found: 2157 phrases; correct: 2126.\n",
      "\n",
      "precision:  98.56%; recall:  97.48%; FB1:  98.02\n",
      "\n",
      "\tLOC: precision:  99.20%; recall:  96.24%; F1:  97.70 1006\n",
      "\n",
      "\tPER: precision:  98.00%; recall:  98.60%; F1:  98.30 1151\n",
      "\n",
      "\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 258 phrases; correct: 154.\n",
      "\n",
      "precision:  59.69%; recall:  51.85%; FB1:  55.50\n",
      "\n",
      "\tLOC: precision:  74.77%; recall:  55.33%; F1:  63.60 111\n",
      "\n",
      "\tPER: precision:  48.30%; recall:  48.30%; F1:  48.30 147\n",
      "\n",
      "\n",
      "Eval on test:\n",
      "processed 28668 tokens with 763 phrases; found: 649 phrases; correct: 338.\n",
      "\n",
      "precision:  52.08%; recall:  44.30%; FB1:  47.88\n",
      "\n",
      "\tLOC: precision:  69.06%; recall:  46.94%; F1:  55.90 278\n",
      "\n",
      "\tPER: precision:  39.35%; recall:  41.24%; F1:  40.28 371\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# net = NER(corp, token_embeddings_dim=300, use_crf=True, char_embeddings_dim=25,\n",
    "# concat_embeddings=True, use_char_embeddins=True)\n",
    "# learning_params = {'dropout_rate': 0.5,\n",
    "#                    'epochs': 20,\n",
    "#                    'learning_rate': 0.005,\n",
    "#                    'batch_size': 1}\n",
    "\n",
    "# results = net.fit(**learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(sentence, network):\n",
    "    tokens = tokenize(sentence)\n",
    "    if len(tokens) == 0:\n",
    "        tokens = ['']    \n",
    "    tags = network.predict_for_token_batch([tokens])[0]\n",
    "    return tokens, tags\n",
    "\n",
    "def print_predict(tokens, tags):\n",
    "    NERS = []\n",
    "    k = 0\n",
    "    curr_ner = []\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag != 'O' and len(token) >= 2:\n",
    "            curr_ner.append(token)\n",
    "        else:\n",
    "            if len(curr_ner) > 0:\n",
    "                NERS.append(\" \".join(curr_ner))\n",
    "            curr_ner = []\n",
    "    if len(curr_ner) > 0:\n",
    "        NERS.append(\" \".join(curr_ner))\n",
    "    return set(NERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = \"Мария сдает работу Ивану Захарову в МФТИ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_predict(S, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Мария', 'сдает', 'работу', 'Ивану', 'Захарову', 'в', 'МФТИ'],\n",
       " ['B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ивану Захарову', 'МФТИ', 'Мария'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_predict(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = ['.', '!', '&', '?', '...', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130822 РГ КП Неретин.txt ___________________________________________________________________________________________\n",
      "Салтыковкой\n",
      "Ленинке\n",
      "Гугла\n",
      "Наверняка\n",
      "Московский\n",
      "Достоевского\n",
      "Пермском хореографическом\n",
      "Инфраструктура\n",
      "Екатеринбурге\n",
      "Строгановка\n",
      "Московских\n",
      "Расфасовав\n",
      "Питерский\n",
      "Большой театр\n",
      "Интересная пропорция\n",
      "Белгородской области\n",
      "Новгородке\n",
      "Пушкинском музее\n",
      "Пентхаус\n",
      "Москву\n",
      "России\n",
      "Второй\n",
      "Кучкаров\n",
      "Академия Глазунова\n",
      "Профессии\n",
      "Статья\n",
      "Попса\n",
      "Забыл\n",
      "СССР\n",
      "Свинарка\n",
      "Рекламу Винзавода\n",
      "Дифференцируем\n",
      "ВВП\n",
      "Петербурге\n",
      "Российской\n",
      "Рубенса\n",
      "Московская\n",
      "Марату Гельману\n",
      "ДШИ\n",
      "Юрий Иванович\n",
      "Риме\n",
      "Кемерово\n",
      "Реставраторы\n",
      "Хорошо\n",
      "Руководством\n",
      "Школа Табакова\n",
      "Кажется\n",
      "Репинка\n",
      "Ленинкой\n",
      "Абстракции\n",
      "ВГИК\n",
      "движимого\n",
      "Сеть культовых\n",
      "Центральной\n",
      "ГИТИСе\n",
      "Санкт\n",
      "Руководитель\n",
      "например\n",
      "Москве\n",
      "59\n",
      "20130910 КП интервью Смирнов.txt ___________________________________________________________________________________________\n",
      "Израиля\n",
      "Ливана читал\n",
      "Салман Рушди\n",
      "Александр Матросов\n",
      "завершить\n",
      "Калягина\n",
      "Плясать\n",
      "Европе\n",
      "Сколково\n",
      "СПб\n",
      "английски\n",
      "Казахстане\n",
      "Голливуд\n",
      "Саудовской Аравии\n",
      "Идеал недостижим\n",
      "Колупаева\n",
      "Ким Ир Сена\n",
      "Марк Розовский\n",
      "России\n",
      "ЗАК\n",
      "СГА\n",
      "Пастернак\n",
      "Израиле\n",
      "Сталин\n",
      "Конституционно\n",
      "Антона Николаевича\n",
      "Архангельской области\n",
      "Фокин\n",
      "Россию\n",
      "Евгений Онегин Пушкина\n",
      "ЮИГ\n",
      "Северной Корее\n",
      "Лев Евгеньевич\n",
      "Михалков\n",
      "Марк Захаров\n",
      "35\n",
      "20130924 РГ КП Бак.txt ___________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir('test_set'):\n",
    "    print(name, \"___________________________________________________________________________________________\")\n",
    "    NERS = set()\n",
    "    with open('test_set/' + name) as f:\n",
    "        for s in f.readlines():\n",
    "            if s == \"\\n\" or s == \" \" or s == \"\\t\":\n",
    "                continue\n",
    "            pred=make_predict(s, net)\n",
    "            NERS = NERS | print_predict(pred[0], pred[1])             \n",
    "    f = open('result/' + name, mode='a')\n",
    "    k = 0\n",
    "    for ner in NERS:\n",
    "        if ner not in points and len(ner) > 2 and not ner.isdigit(): \n",
    "            print(ner)\n",
    "            k += 1\n",
    "            f.write(ner + \"\\n\")\n",
    "    print(k)\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
