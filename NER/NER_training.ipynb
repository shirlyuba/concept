{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.utils import download_untar\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ner.network import NER\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "from ner.utils import tokenize, lemmatize\n",
    "\n",
    "\n",
    "from os.path import join\n",
    "folder = join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c33d710eeb4c09abc9de193bdc03f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with open(join('.', 'NER_concept.txt'), encoding='utf-8') as f:\n",
    "#     xy_list = list()\n",
    "#     tokens = list()\n",
    "# #     tags = list()\n",
    "#     val = list()\n",
    "#     for line in tqdm_notebook(f):\n",
    "#         items = line.split()\n",
    "#         if len(items) > 1 and '-DOCSTART-' not in items[0]:\n",
    "#             token, tag = items\n",
    "#             token_ = tokenize(token)\n",
    "#             if len(token_) > 0:\n",
    "#                 token = token_[0]\n",
    "#             if token[0].isdigit():\n",
    "#                 tokens.append('#')\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#             tags.append(tag)\n",
    "#         elif len(tokens) > 0:\n",
    "#             xy_list.append((tokens, tags,))\n",
    "#             tokens = list()\n",
    "#             tags = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_list = np.load('NER_set_simple.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_list = np.append(xy_list, np.load('NER_set_conll_2003.npy'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['train'], ost = train_test_split(xy_list,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['valid'], dataset_dict['test'] = train_test_split(ost,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('NER_set_simple.npy', xy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('NER_set_conll_2003.npy', xy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ner.corpus import Corpus\n",
    "corp = Corpus(dataset_dict, embeddings_file_path='model185.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('my_model/params.json') as f:\n",
    "    network_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_model/my_model\n"
     ]
    }
   ],
   "source": [
    "net = NER(corp, verbouse=False, pretrained_model_filepath='my_model/my_model', **network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "Embeddings 5826150\n",
      "ConvNet 338688\n",
      "Classifier 1542\n",
      "transitions:0 36\n",
      "Total number of parameters equal 6166416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 277 phrases; correct: 173.\n",
      "\n",
      "precision:  62.45%; recall:  58.25%; FB1:  60.28\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 224 phrases; correct: 177.\n",
      "\n",
      "precision:  79.02%; recall:  59.60%; FB1:  67.95\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 239 phrases; correct: 185.\n",
      "\n",
      "precision:  77.41%; recall:  62.29%; FB1:  69.03\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 329 phrases; correct: 158.\n",
      "\n",
      "precision:  48.02%; recall:  53.20%; FB1:  50.48\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 231 phrases; correct: 161.\n",
      "\n",
      "precision:  69.70%; recall:  54.21%; FB1:  60.98\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 276 phrases; correct: 190.\n",
      "\n",
      "precision:  68.84%; recall:  63.97%; FB1:  66.32\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 319 phrases; correct: 190.\n",
      "\n",
      "precision:  59.56%; recall:  63.97%; FB1:  61.69\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 313 phrases; correct: 197.\n",
      "\n",
      "precision:  62.94%; recall:  66.33%; FB1:  64.59\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 226 phrases; correct: 160.\n",
      "\n",
      "precision:  70.80%; recall:  53.87%; FB1:  61.19\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 240 phrases; correct: 166.\n",
      "\n",
      "precision:  69.17%; recall:  55.89%; FB1:  61.82\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 381 phrases; correct: 174.\n",
      "\n",
      "precision:  45.67%; recall:  58.59%; FB1:  51.33\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 197 phrases; correct: 136.\n",
      "\n",
      "precision:  69.04%; recall:  45.79%; FB1:  55.06\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 214 phrases; correct: 154.\n",
      "\n",
      "precision:  71.96%; recall:  51.85%; FB1:  60.27\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 211 phrases; correct: 151.\n",
      "\n",
      "precision:  71.56%; recall:  50.84%; FB1:  59.45\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 201 phrases; correct: 148.\n",
      "\n",
      "precision:  73.63%; recall:  49.83%; FB1:  59.44\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 248 phrases; correct: 164.\n",
      "\n",
      "precision:  66.13%; recall:  55.22%; FB1:  60.18\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 180 phrases; correct: 145.\n",
      "\n",
      "precision:  80.56%; recall:  48.82%; FB1:  60.80\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 206 phrases; correct: 143.\n",
      "\n",
      "precision:  69.42%; recall:  48.15%; FB1:  56.86\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 313 phrases; correct: 180.\n",
      "\n",
      "precision:  57.51%; recall:  60.61%; FB1:  59.02\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 258 phrases; correct: 154.\n",
      "\n",
      "precision:  59.69%; recall:  51.85%; FB1:  55.50\n",
      "\n",
      "\n",
      "Eval on train:\n",
      "processed 83191 tokens with 2181 phrases; found: 2157 phrases; correct: 2126.\n",
      "\n",
      "precision:  98.56%; recall:  97.48%; FB1:  98.02\n",
      "\n",
      "\tLOC: precision:  99.20%; recall:  96.24%; F1:  97.70 1006\n",
      "\n",
      "\tPER: precision:  98.00%; recall:  98.60%; F1:  98.30 1151\n",
      "\n",
      "\n",
      "Eval on valid:\n",
      "processed 11361 tokens with 297 phrases; found: 258 phrases; correct: 154.\n",
      "\n",
      "precision:  59.69%; recall:  51.85%; FB1:  55.50\n",
      "\n",
      "\tLOC: precision:  74.77%; recall:  55.33%; F1:  63.60 111\n",
      "\n",
      "\tPER: precision:  48.30%; recall:  48.30%; F1:  48.30 147\n",
      "\n",
      "\n",
      "Eval on test:\n",
      "processed 28668 tokens with 763 phrases; found: 649 phrases; correct: 338.\n",
      "\n",
      "precision:  52.08%; recall:  44.30%; FB1:  47.88\n",
      "\n",
      "\tLOC: precision:  69.06%; recall:  46.94%; F1:  55.90 278\n",
      "\n",
      "\tPER: precision:  39.35%; recall:  41.24%; F1:  40.28 371\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# net = NER(corp, token_embeddings_dim=300, use_crf=True, char_embeddings_dim=25,\n",
    "# concat_embeddings=True, use_char_embeddins=True)\n",
    "# learning_params = {'dropout_rate': 0.5,\n",
    "#                    'epochs': 20,\n",
    "#                    'learning_rate': 0.005,\n",
    "#                    'batch_size': 1}\n",
    "\n",
    "# results = net.fit(**learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(sentence, network):\n",
    "    tokens = tokenize(sentence)\n",
    "    if len(tokens) == 0:\n",
    "        tokens = ['']    \n",
    "    tags = network.predict_for_token_batch([tokens])[0]\n",
    "    return tokens, tags\n",
    "\n",
    "def print_predict(tokens, tags):\n",
    "    NERS = []\n",
    "    k = 0\n",
    "    curr_ner = []\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag != 'O' and len(token) >= 2:\n",
    "            curr_ner.append(token)\n",
    "        else:\n",
    "            if len(curr_ner) > 0:\n",
    "                NERS.append(\" \".join(curr_ner))\n",
    "            curr_ner = []\n",
    "    if len(curr_ner) > 0:\n",
    "        NERS.append(\" \".join(curr_ner))\n",
    "    return set(NERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = \"Мария сдает работу Ивану Захарову в МФТИ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_predict(S, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Мария', 'сдает', 'работу', 'Ивану', 'Захарову', 'в', 'МФТИ'],\n",
       " ['B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ивану Захарову', 'МФТИ', 'Мария'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_predict(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = ['.', '!', '&', '?', '...', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130822 РГ КП Неретин.txt ___________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Школа Табакова\n",
      "Профессии\n",
      "Пушкинском музее\n",
      "Большой театр\n",
      "Российской\n",
      "Сеть культовых\n",
      "Инфраструктура\n",
      "Санкт\n",
      "Питерский\n",
      "Свинарка\n",
      "Кучкаров\n",
      "Строгановка\n",
      "Московских\n",
      "Расфасовав\n",
      "ДШИ\n",
      "Гугла\n",
      "например\n",
      "Кажется\n",
      "Пентхаус\n",
      "Московская\n",
      "Наверняка\n",
      "Екатеринбурге\n",
      "Ленинкой\n",
      "Интересная пропорция\n",
      "Хорошо\n",
      "Рекламу Винзавода\n",
      "ВГИК\n",
      "Руководитель\n",
      "Второй\n",
      "Новгородке\n",
      "Достоевского\n",
      "Забыл\n",
      "Москву\n",
      "Попса\n",
      "Реставраторы\n",
      "Репинка\n",
      "СССР\n",
      "Московский\n",
      "Белгородской области\n",
      "Дифференцируем\n",
      "Петербурге\n",
      "ГИТИСе\n",
      "Руководством\n",
      "Академия Глазунова\n",
      "России\n",
      "Юрий Иванович\n",
      "Статья\n",
      "Кемерово\n",
      "движимого\n",
      "Салтыковкой\n",
      "Риме\n",
      "Ленинке\n",
      "Москве\n",
      "ВВП\n",
      "Центральной\n",
      "Рубенса\n",
      "Пермском хореографическом\n",
      "Марату Гельману\n",
      "Абстракции\n",
      "20130910 КП интервью Смирнов.txt ___________________________________________________________________________________________\n",
      "Северной Корее\n",
      "английски\n",
      "Колупаева\n",
      "Евгений Онегин Пушкина\n",
      "ЗАК\n",
      "Израиле\n",
      "Салман Рушди\n",
      "Конституционно\n",
      "Сталин\n",
      "Плясать\n",
      "ЮИГ\n",
      "Архангельской области\n",
      "Фокин\n",
      "Казахстане\n",
      "СПб\n",
      "Идеал недостижим\n",
      "Израиля\n",
      "Пастернак\n",
      "Россию\n",
      "Голливуд\n",
      "Антона Николаевича\n",
      "Марк Захаров\n",
      "Калягина\n",
      "России\n",
      "завершить\n",
      "Михалков\n",
      "Саудовской Аравии\n",
      "Европе\n",
      "Марк Розовский\n",
      "Александр Матросов\n",
      "Лев Евгеньевич\n",
      "Ливана читал\n",
      "Сколково\n",
      "Ким Ир Сена\n",
      "СГА\n",
      "20130924 РГ КП Бак.txt ___________________________________________________________________________________________\n",
      "Инсарова\n",
      "Кирилл Эмильевич Разлогов\n",
      "людей\n",
      "Демьяна Бедного\n",
      "Ролан Барт\n",
      "Лиснянская\n",
      "Герцен\n",
      "Акунин\n",
      "Вигель\n",
      "Анна Каренина\n",
      "Госполитика\n",
      "Щедрин\n",
      "например\n",
      "Булгаринское\n",
      "Идиота\n",
      "Звенья\n",
      "Третьяковку\n",
      "Университета\n",
      "Константин Николаевич\n",
      "Анны Карениной\n",
      "Гоголе\n",
      "марионетка\n",
      "Ливерстрос\n",
      "Анну Каренину\n",
      "Карамзин\n",
      "Братьев Карамазовых\n",
      "Лихачев\n",
      "Василий Григорьевич Овсеенко\n",
      "Другая\n",
      "Донцову\n",
      "Разлогов\n",
      "Никсон\n",
      "Москве\n",
      "Шойгу\n",
      "Фета\n",
      "Актер\n",
      "ТОО Росимущество\n",
      "Некрасов публикует Добролюбова\n",
      "МФТИ\n",
      "Островский\n",
      "Готов согласиться\n",
      "Донцова\n",
      "Кучкаров\n",
      "Москву Пушкина\n",
      "Люблю Россию\n",
      "Чехов\n",
      "Марина Васильевна\n",
      "Пришвин\n",
      "Лермонтов\n",
      "Андрей Зорин\n",
      "архив\n",
      "Наталье Владимировне Шахаловой\n",
      "Улюкаев\n",
      "Сталин\n",
      "Госполитика эффективна\n",
      "Булгарин\n",
      "Путина\n",
      "Булгариным\n",
      "Буркхард\n",
      "Достоевский\n",
      "монастыря\n",
      "Степанова\n",
      "Достоевского\n",
      "Луначарский\n",
      "Павла\n",
      "Георгия Димитрова\n",
      "Молчановке\n",
      "Бориса Годунова\n",
      "Тургенев\n",
      "Капри\n",
      "творОг\n",
      "нАчать нАчал\n",
      "Каткова\n",
      "Андрея Скача\n",
      "Ясной поляне\n",
      "Аксаков\n",
      "ВВП\n",
      "Ивана Павлова\n",
      "Жорж Санд\n",
      "Поэзия\n",
      "Ad Marginem\n",
      "Деррида\n",
      "Карла Аймермахера\n",
      "Пушкина\n",
      "Горький\n",
      "Брюсов\n",
      "Гумилева\n",
      "Потребности\n",
      "Путиным\n",
      "Некрасов\n",
      "Андрей Александрович\n",
      "Российской\n",
      "Улицкая\n",
      "Красную\n",
      "Пуссен\n",
      "Невы\n",
      "Николай пригласил\n",
      "Надеждин\n",
      "Петербурга\n",
      "Смешно\n",
      "Белинского\n",
      "Александр Филиппович Смирдин\n",
      "Александр Терентьевич Иванов\n",
      "Бога\n",
      "Бруевич\n",
      "Настолько\n",
      "Натальи Николаевны\n",
      "Анны Павловны Шерер\n",
      "Гербера\n",
      "Россия\n",
      "Рим\n",
      "Бассманной\n",
      "Филатов Сергей Александрович\n",
      "Скажу\n",
      "Авторство Бака\n",
      "Литмузей\n",
      "Девки\n",
      "Юрий Денисюк\n",
      "Правильных коллективов\n",
      "Наследникам\n",
      "журнале\n",
      "Николай Павлович\n",
      "Катков\n",
      "Тургеневу\n",
      "Жанейро\n",
      "Толстой\n",
      "Алексей Александрович Каренин\n",
      "Константин\n",
      "Англии\n",
      "Лев Толстой\n",
      "Казань\n",
      "Николай Иванович Надеждин\n",
      "Бонч\n",
      "Василий Львович Пушкин\n",
      "Современника\n",
      "Мединский\n",
      "Алексей Чанцев\n",
      "Некрасова\n",
      "Гагариной\n",
      "Рио\n",
      "Пьера Бурдьё\n",
      "Константин Черный\n",
      "Толстым\n",
      "Тургеневе\n",
      "Солженицына\n",
      "Писатель\n",
      "ЯВС\n",
      "Бог\n",
      "Салтыков\n",
      "Невском\n",
      "Белинский\n",
      "Горбачев\n",
      "Шамиля\n",
      "МХАТовцы\n",
      "Жуковский\n",
      "Катков Михаил Никифорович\n",
      "Раймон Арон\n",
      "Пастернак\n",
      "Россию\n",
      "Дворец Советов\n",
      "Карабихи\n",
      "Павел Пажедаев\n",
      "России\n",
      "Михаил Никифорович\n",
      "Тютчев\n",
      "Евтушенко\n",
      "Пушкину\n",
      "Северная\n",
      "Земля\n",
      "Пушкин\n",
      "Империи\n",
      "Даль\n",
      "Эйзенштейн\n",
      "Толстого\n",
      "Петровке 28\n",
      "Бедной Лизы\n",
      "Петербург\n",
      "Григорович\n",
      "Ленин\n",
      "Госдума\n",
      "Чехова\n",
      "империи\n",
      "20130928 РГ КП Фокин.txt ___________________________________________________________________________________________\n",
      "драму\n",
      "Италию\n",
      "делать театр\n",
      "Часть\n",
      "Ставящие\n",
      "Сундстрем\n",
      "Никуда\n",
      "Валерию Владимировичу\n",
      "Воронежского театра кукол\n",
      "Дмитрий\n",
      "Николай говорил\n",
      "Драматический театр\n",
      "Германии\n",
      "Ленинградской области\n",
      "Режиссёры\n",
      "Николай\n",
      "Фокин\n",
      "Мариинка\n",
      "театр\n",
      "разному\n",
      "Мейерхольд\n",
      "Кушать\n",
      "Искусств\n",
      "Юрий Иванович\n",
      "Чего\n",
      "Ревизор\n",
      "удовольствием\n",
      "Москве\n",
      "Воронежского\n",
      "Понимаешь\n",
      "Путин\n",
      "Гоголь\n",
      "Хохлов\n",
      "Гаврилов\n",
      "Докладывается Президенту\n",
      "Провозглашать подъём\n",
      "Счётную\n",
      "Мединский\n",
      "Маргарет Мид\n",
      "зданию театр\n",
      "Питере\n",
      "министром\n",
      "Кучкаров\n",
      "Чехов\n",
      "Окурки\n",
      "Трубочкин\n",
      "Юра\n",
      "театра\n",
      "Предлагать\n",
      "реквизиторский\n",
      "Воронежскую\n",
      "Гоголя\n",
      "Специальные приёмы\n",
      "Валерий Владимирович\n",
      "Чиновник\n",
      "Бог\n",
      "Ревизора\n",
      "Счастье\n",
      "имени Кольцова\n",
      "Павел Корчагин\n",
      "Островского\n",
      "Эрмитаж\n",
      "Петербурге\n",
      "help\n",
      "России\n",
      "Гамлет\n",
      "Путину\n",
      "Европе\n",
      "Гергиева\n",
      "площади\n",
      "Сухарев\n",
      "Толстого\n",
      "Малый театр\n",
      "Тольяттинский\n",
      "Спектакль\n",
      "Юрий Мефодьевич\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir('test_set'):\n",
    "    print(name, \"___________________________________________________________________________________________\")\n",
    "    NERS = set()\n",
    "    with open('test_set/' + name) as f:\n",
    "        for s in f.readlines():\n",
    "            if s == \"\\n\" or s == \" \" or s == \"\\t\":\n",
    "                continue\n",
    "            pred=make_predict(s, net)\n",
    "            NERS = NERS | print_predict(pred[0], pred[1])             \n",
    "    f = open('result/' + name, mode='a')\n",
    "    for ner in NERS:\n",
    "        if ner not in points and len(ner) > 2 and not ner.isdigit(): \n",
    "            print(ner)\n",
    "            f.write(ner + \"\\n\")\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
